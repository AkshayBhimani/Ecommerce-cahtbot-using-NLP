# -*- coding: utf-8 -*-
"""Combined_scraper_and_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lzwDRWNx3id094VJyFsBMIG2uak9XIZW

##scraping
"""

# pip install fake_useragent

import urllib
from urllib.parse import urlparse
import requests
from fake_useragent import UserAgent
from bs4 import BeautifulSoup
import re   
import pandas as pd
import re   

reviewlist =[]

def getAll(productName):
    print()
    #  = input("product name :")
    query = productName+"'amazon.in'" 
    query = urllib.parse.quote_plus(query)
    number_result = 20

    ua = UserAgent()

    google_url = "https://www.google.com/search?q=" + query + "&num=" + str(number_result)
    response = requests.get(google_url, {"User-Agent": ua.random})
    soup = BeautifulSoup(response.text, "html.parser")

    result_div = soup.find_all('div', attrs = {'class': 'ZINbbc'})

    links = []
    titles = []
    descriptions = []
    for r in result_div:
        try:
            link = r.find('a', href = True)
            title = r.find('div', attrs={'class':'vvjwJb'}).get_text()
            description = r.find('div', attrs={'class':'s3v9rd'}).get_text()

            if link != '' and title != '' and description != '': 
                links.append(link['href'])
                titles.append(title)
                descriptions.append(description)
        except:
            continue


    to_remove = []
    clean_links = []
    for i, l in enumerate(links):
        clean = re.search('\/url\?q\=(.*)\&sa',l)

        if clean is None:
            to_remove.append(i)
            continue
        clean_links.append(clean.group(1))

    # for x in to_remove:
    #     del titles[x]
    #     del descriptions[x]

    # print()
    # print('link :', clean_links[0])
    # print()
    productLink = clean_links[0]
    # print(clean_links)
    productReviewsLink = productLink.replace('/dp/','/product-reviews/')
    # print(productReviewsLink)


    def get_soup(URL):
        
        HEADERS = ({'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36','Accept-Language': 'en-US, en;q=0.5'})
        r = requests.get(URL, headers=HEADERS)

        soup = BeautifulSoup(r.text, 'html.parser')
        return soup

    # cleanLink()

    def get_reviews(soup):
        reviews = soup.find_all('div', {'data-hook': 'review'})
        try:
            for item in reviews:
                review = {
                'product': soup.title.text.replace('Amazon.co.uk:Customer reviews:', '').strip(),
                'title': item.find('a', {'data-hook': 'review-title'}).text.strip(),
                'rating':  float(item.find('i', {'data-hook': 'review-star-rating'}).text.replace('out of 5 stars', '').strip()),
                'body': item.find('span', {'data-hook': 'review-body'}).text.strip(),
                }
                reviewlist.append(review)
        except:
            pass





    for x in range(1,10):
        if x==1:
            reviewPageLink=str(f''+productReviewsLink)+str(f'/ref=cm_cr_dp_d_show_all_btm'+f'?ie=UTF8&reviewerType=all_reviews')
   
        else:
            reviewPageLink=str(f''+productReviewsLink)+str(f'/ref=cm_cr_dp_d_show_all_btm_next_{x}'+f'?ie=UTF8&reviewerType=all_reviews&pageNumber={x}')
        # print(reviewPageLink)
        soup = get_soup(reviewPageLink)
        # print(f'Getting page: {x}')
        # print(soup)
        get_reviews(soup)
        # print(len(reviewlist))
        if not soup.find('li', {'class': 'a-disabled a-last'}):
            pass
        else:
            break
    

    df = pd.DataFrame(reviewlist)
    print(list(df.columns))
    # df.to_excel(str(productName)+'.xlsx', index=False)
    df_prince = df[['body','rating']]
    # print(df_prince.head(10))
    reviews_for_prince = df['body'].to_list()
    rating_for_prince = df['rating'].to_list()

    return reviews_for_prince, rating_for_prince,productLink


##Starting the part of sentiment analysis here.
import pickle
# import pandas as pd
# import numpy as np
# from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer

# tfidf_vectorizer = TfidfVectorizer(min_df=5, max_df=0.8)


def get_reviews(prod_name):
  tfidf_vectorizer = pickle.load(open('Combined_scraper_and_sentiment_analysis/tfidf_vectorizer.pkl', 'rb'))

  loaded_model = pickle.load(open('Combined_scraper_and_sentiment_analysis/svm_rbf.pkl', 'rb'))

  x = ""
  x1 = ""
  reviews, ratings,link = getAll(prod_name)
  flags = list()

  cnt = 1
  for r in reviews:
    new_test_transform_tfidf = tfidf_vectorizer.transform([r])

    if loaded_model.predict(new_test_transform_tfidf):
      flags.append(1)
    else: 
      flags.append(0)
    cnt += 1

  print(flags)
  ultimate_sentiment = sum(flags)
  print(ultimate_sentiment)
  ultimate_rating = sum(flags)/len(flags)
  print(ultimate_rating, '% positive')
  postive_review = ultimate_rating*100
  if ultimate_rating < 0.5:
    from_review = (f"You should not buy {prod_name}")
  else: 
    from_review = (f"You should buy {prod_name}")

  according_to_rating = sum(ratings)/len(ratings)
  postive_rating = according_to_rating
  print(according_to_rating)
  if ultimate_rating > 3:
    from_rating= ("You should not buy this.")
  else: 
    from_rating =("You should buy this.")
  return from_review,from_rating,postive_review,postive_rating,link

# zz,zv = get_reviews("samsung s21 ulta")
# print("zz zv",zz,zv)